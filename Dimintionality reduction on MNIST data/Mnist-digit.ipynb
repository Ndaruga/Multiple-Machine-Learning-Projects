{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reducion on MNIST digit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install python-mnist\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "(X_train,y_train),(X_test,y_test) = mnist.load_data()\n",
    "print(\"Train data\")\n",
    "print(f\"X Train: {X_train.shape}\")\n",
    "print(f\"y train: {y_train.shape}\")\n",
    "print(\"*\"*100)\n",
    "print(\"Test data\")\n",
    "print(f\"X Test: {X_test.shape}\")\n",
    "print(f\"Y Text: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data above, we can already tell a few things\n",
    "<li>The Train and test data has 60000 and 10000 images respectively</li>\n",
    "<li>The images have a dimention of 28 by 28 pixels</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### join the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = np.vstack((X_train, X_test))\n",
    "print(\"X data: {} \".format(X_data.shape))\n",
    "\n",
    "Y_data = np.hstack((y_train, y_test))\n",
    "print(\"Y data: {} \".format(Y_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(Y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have numpy arrays of X_data and Y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first image on the data without alteing the colour\n",
    "\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "plt.title(\"Image 1\")\n",
    "plt.imshow(X_data[0])\n",
    "plt.show()\n",
    "\n",
    "# display(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above images show that the digit images are in <strong>RGB colour channels</strong> and we need to turn them into grayscale\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  We can go further to plot the first few images\n",
    "\n",
    "images = X_data[0:14]\n",
    "labels = Y_data[0:14]\n",
    "fig,axes = plt.subplots(2,7, figsize = (16,5))\n",
    "\n",
    "for img in range(14):\n",
    "    ax = axes[img//7, img%7]\n",
    "    ax.imshow(images[img], cmap=plt.cm.binary) # we show the digits in a binary colour\n",
    "    ax.set_title(\"Label: {}\".format(labels[img]))\n",
    "plt.tight_layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lets see how the colour digits are distributed on the first image on the data.\n",
    "for i in X_data[1]:\n",
    "    print(i.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly seee that this is number 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reashaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are interesed in dimentionality reduction, <br>we will need to reshape the data arrays into a <strong>single colour channel</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the X_train and X_test\n",
    "\n",
    "display(X_data.shape)\n",
    "X_data = X_data.reshape((X_data.shape[0], 28,28,-1))\n",
    "\n",
    "\n",
    "print(f\"After reshaping, X_data dim: {X_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming the data into High dimention\n",
    "<li>Part 1: Use the 10 categories and see the result</li>\n",
    "<li>Part 2: Use the 784 fratures <strong>28 X 28 </strong> and see the result</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Using the 10 categorical digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encodding and representation\n",
    "lr = np.arange(10)\n",
    "\n",
    "for label in range(10):\n",
    "    one_hot = (lr==label).astype(np.int)\n",
    "    print(\"label: \", label, \" in one-hot representation: \", one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Onehot encodding\n",
    "encodded_data = pd.get_dummies(Y_data)\n",
    "print(encodded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above we can see that the encoded data shape after one hot encoding we get 10 categories. <br>\n",
    "This is because the digits range from 0 -9 which are 10 10 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_data)\n",
    "print('-'*20)\n",
    "print(len(Y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the y_data numpy array to a pandas dataframe with column label\n",
    "\n",
    "Y_df = pd.DataFrame(Y_data, columns=['labels'])\n",
    "Y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizaton using t-SNE\n",
    "\n",
    "T-Dristribted Stochastic Neighbour Embedding (t-SNE) is a non linear dimentionality reduction techique that well suits vizualization of high dimension data especially on a 2D plot using conditional probalbility.<br>\n",
    "t-SNE uses Gausian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converng data into a pandas dataframe for easier handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the one hot encoded data into a data frame and the concating labels \n",
    "\n",
    "enc_df = pd.DataFrame(encodded_data)\n",
    "print(pd.concat([enc_df, Y_df], axis=1))\n",
    "\n",
    "# print('-'*100)\n",
    "# display(train_df.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute t-SNE \n",
    "\n",
    "# def tSNE(dataframe):\n",
    "start_time = time.time()\n",
    "tsne = TSNE(n_components = 2, verbose=0, learning_rate=200, n_iter=500)\n",
    "tsne_result = tsne.fit_transform(X=enc_df)\n",
    "end_time = time.time()\n",
    "print(\"Learning completed in {} seconds\".format(end_time - start_time))\n",
    "\n",
    "    # return tsne_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df1 = pd.DataFrame({\"t-SNE 1\":tsne_result[:,0], \"t-SNE 2\":tsne_result[:,1], \"label\":Y_df['labels']})\n",
    "tsne_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the tsne data\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(15,10))\n",
    "sns.scatterplot(x = 't-SNE 1', y= 't-SNE 2', hue = 'label', data = tsne_df1, ax = ax, s=20, palette = 'dark')\n",
    "sns.color_palette(\"hls\", 10)\n",
    "lim = (tsne_result.min()-5, tsne_result.max()+5)\n",
    "ax.set_xlim(lim)\n",
    "ax.set_ylim(lim)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title('Part 1: t-SNE Visualization', fontsize = 16, weight = 'bold')\n",
    "ax.legend(bbox_to_anchor = (1,1), loc =2, borderaxespad = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Using the 784 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening the array from 2D to 1D\n",
    "\n",
    "X_data2 = []\n",
    "for i in range(len(X_data)):\n",
    "    X_data2.append(X_data[i].flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convering into a dataframe\n",
    "\n",
    "X_data2 = pd.DataFrame(X_data2, columns = [str(i) for i in range(28*28)])\n",
    "X_data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "tsne2 = TSNE(n_components = 2)\n",
    "tsne_result2 = tsne2.fit_transform(X_data2)\n",
    "end_time = time.time()\n",
    "print(\"Learning completed in {} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tsne_result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df2 = pd.DataFrame({\"t-SNE 1\":tsne_result2[:,0], \"t-SNE 2\":tsne_result2[:,1], \"label\":Y_df['labels']})\n",
    "fig, ax = plt.subplots(1, figsize=(15,10))\n",
    "sns.scatterplot(x = 't-SNE 1', y= 't-SNE 2', hue = 'label', data = tsne_df2, ax = ax, s=20, palette = 'bright')\n",
    "# sns.color_palette('bright', 10)\n",
    "lim = (tsne_result2.min()-5, tsne_result2.max()+5)\n",
    "ax.set_xlim(lim)\n",
    "ax.set_ylim(lim)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title('Part 2: t-SNE Visualization with 784 features', fontsize = 16, weight = 'bold')\n",
    "ax.legend(bbox_to_anchor = (1,1), loc =2, borderaxespad = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vizualization using SDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same Degree Distribution (SDD) is dimentionality reduction approach that clusters non-linear data stuctures.<br>\n",
    "SDD will try to determine which degree of distribution can best capture non-linear data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from scipy import linalg\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.sparse import csr_matrix, issparse\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils._openmp_helpers import _openmp_effective_n_threads\n",
    "from sklearn.utils.validation import check_non_negative\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "MACHINE_EPSILON = np.finfo(np.double).eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _joint_probabilities(X, degrees_of_freedom):\n",
    "    dist = pdist(X, \"sqeuclidean\")\n",
    "    dist += 1.\n",
    "    dist **= (-degrees_of_freedom)\n",
    "    P= np.maximum(dist / ( np.sum(dist)), MACHINE_EPSILON)\n",
    "    return P\n",
    "\n",
    "\n",
    "def _kl_divergence(params, P, degrees_of_freedom, n_samples, n_components,\n",
    "                  skip_num_points=0, compute_error=True):\n",
    "\n",
    "    X_embedded = params.reshape(n_samples, n_components)\n",
    "    dist = pdist(X_embedded, \"sqeuclidean\")\n",
    "    Dist=dist+ 1.\n",
    "    dist= Dist**(-degrees_of_freedom)\n",
    "    Q = np.maximum(dist / (np.sum(dist)), MACHINE_EPSILON)\n",
    "    dist1=Dist**(-1)\n",
    "    kl_divergence =np.dot(P, np.log(np.maximum(P, MACHINE_EPSILON) / Q))\n",
    "    grad = np.ndarray((n_samples, n_components), dtype=params.dtype)\n",
    "    PQd = squareform((P - Q) * dist1)\n",
    "    for i in range(skip_num_points, n_samples):\n",
    "        grad[i] = np.dot(np.ravel(PQd[i]),\n",
    "                        X_embedded[i] - X_embedded)\n",
    "    grad = grad.ravel()\n",
    "    c = 2*(degrees_of_freedom + 1.0) \n",
    "    grad *= c\n",
    "\n",
    "    return kl_divergence, grad\n",
    "\n",
    "def _gradient_descent(objective, p0, it, n_iter,\n",
    "                     n_iter_check=1, n_iter_without_progress=300,\n",
    "                     momentum=0.5, learning_rate=7, min_gain=0.01,\n",
    "                     min_grad_norm=1e-7, verbose=0, args=None, kwargs=None):\n",
    "    if args is None:\n",
    "        args = []\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "\n",
    "    p = p0.copy().ravel()\n",
    "    update = np.zeros_like(p)\n",
    "    gains = np.ones_like(p)\n",
    "    error = np.finfo(np.float).max\n",
    "    best_error = np.finfo(np.float).max\n",
    "    best_iter = i = it\n",
    "\n",
    "    tic = time.time()\n",
    "    for i in range(it, n_iter):\n",
    "        check_convergence = (i + 1) % n_iter_check == 0\n",
    "       \n",
    "        kwargs['compute_error'] = check_convergence or i == n_iter - 1\n",
    "\n",
    "        error, grad = objective(p, *args, **kwargs)\n",
    "        grad_norm = linalg.norm(grad)\n",
    "\n",
    "        inc = update * grad < 0.0\n",
    "        dec = np.invert(inc)\n",
    "        gains[inc] += 0.2\n",
    "        gains[dec] *= 0.8\n",
    "        np.clip(gains, min_gain, np.inf, out=gains)\n",
    "        grad *= gains\n",
    "        update = momentum * update - learning_rate * grad\n",
    "        p += update\n",
    "\n",
    "        if check_convergence:\n",
    "            toc = time.time()\n",
    "            duration = toc - tic\n",
    "            tic = toc\n",
    "\n",
    "            if verbose >= 2:\n",
    "                print(\"[t-SNE] Iteration %d: error = %.7f,\"\n",
    "                     \" gradient norm = %.7f\"\n",
    "                     \" (%s iterations in %0.3fs)\"\n",
    "                     % (i + 1, error, grad_norm, n_iter_check, duration))\n",
    "\n",
    "            if error < best_error:\n",
    "                best_error = error\n",
    "                best_iter = i\n",
    "            elif i - best_iter > n_iter_without_progress:\n",
    "                if verbose >= 2:\n",
    "                    print(\"[t-SNE] Iteration %d: did not make any progress \"\n",
    "                         \"during the last %d episodes. Finished.\"\n",
    "                         % (i + 1, n_iter_without_progress))\n",
    "                break\n",
    "            if grad_norm <= min_grad_norm:\n",
    "                if verbose >= 2:\n",
    "                    print(\"[t-SNE] Iteration %d: gradient norm %f. Finished.\"\n",
    "                         % (i + 1, grad_norm))\n",
    "                break\n",
    "\n",
    "    return p, error, i\n",
    "\n",
    "\n",
    "class SDD(BaseEstimator):\n",
    "   \n",
    "   \n",
    "    _EXPLORATION_N_ITER = 300\n",
    "    _N_ITER_CHECK =50\n",
    "    def _fit(self, X, degrees_of_freedom, skip_num_points=0):\n",
    "        random_state=None\n",
    "        n_samples=X.shape[0]\n",
    "        P = _joint_probabilities(X,degrees_of_freedom)\n",
    "\n",
    "        random_state = check_random_state(random_state)\n",
    "\n",
    "        X_embedded = 1e-4 * random_state.randn(\n",
    "               n_samples, 2).astype(np.float32)\n",
    "\n",
    "        return self._tsne(P, degrees_of_freedom, n_samples,X_embedded=X_embedded,skip_num_points=skip_num_points)\n",
    "    \n",
    "    def _tsne(self, P, degrees_of_freedom, n_samples, X_embedded, skip_num_points=0):\n",
    "        \"\"\"Runs t-SNE.\"\"\"\n",
    "        params = X_embedded.ravel()\n",
    "\n",
    "        opt_args = {\n",
    "           \"it\": 0,\n",
    "           \"n_iter_check\": 50,\n",
    "           \"min_grad_norm\": 1e-7,\n",
    "           \"learning_rate\": 7,\n",
    "           \"verbose\": 0,\n",
    "           \"kwargs\": dict(skip_num_points=skip_num_points),\n",
    "           \"args\": [P, degrees_of_freedom, n_samples, 2],\n",
    "           \"n_iter_without_progress\": 300,\n",
    "           \"n_iter\": 300,\n",
    "           \"momentum\": 0.8,\n",
    "       }\n",
    "        obj_func = _kl_divergence\n",
    "\n",
    "        params, kl_divergence, it = _gradient_descent(obj_func, params,\n",
    "                                                     **opt_args)\n",
    "        P /= 1\n",
    "        remaining =2000 - 300\n",
    "        if it < 300 or remaining > 0:\n",
    "            opt_args['n_iter'] = 2000\n",
    "            opt_args['it'] = it + 1\n",
    "            opt_args['momentum'] = 0.8\n",
    "            opt_args['n_iter_without_progress'] = 300\n",
    "            params, kl_divergence, it = _gradient_descent(obj_func, params,\n",
    "                                                         **opt_args)\n",
    "\n",
    "        self.n_iter_ = it\n",
    "        X_embedded = params.reshape(n_samples, 2)\n",
    "        self.kl_divergence_ = kl_divergence\n",
    "\n",
    "        return X_embedded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "kendSDD=[]\n",
    "\n",
    "import scipy\n",
    "A=scipy.spatial.distance.pdist(X_df1, metric='euclidean')\n",
    "for i in range(1,10):\n",
    "    start = time.time()\n",
    "    embedding = SDD()\n",
    "    X_SDD = embedding._fit(X_df1, degrees_of_freedom=i)\n",
    "    B=scipy.spatial.distance.pdist(X_SDD1, metric='euclidean')\n",
    "    kendSDD1.append(scipy.stats.kendalltau(A, B))\n",
    "    stop=time.time()\n",
    "    print(f\"Embedded loop {i} in {stop-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kendSDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "embedding=SDD()\n",
    "X_SDD = embedding._fit(X_df1,degrees_of_freedom=(kendSDD1.index(max(kendSDD1))+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "04b2e48da8715d0e03ed003238e610aa18fc3abec3b418b1e66df75f2c0e936e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
