{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6de422b4",
   "metadata": {},
   "source": [
    "# London Fire Incidents Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa7d355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26028f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f4583b",
   "metadata": {},
   "source": [
    "### Downloading and loadng the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7428026",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "# import data_download\n",
    "\n",
    "data_dir = \"LFB-data\"\n",
    "# LFB_data = pd.read_csv(os.path.join(data_dir, \"LFB Incident data - Datastore - with notional cost and UPRN from January 2009.csv\"))\n",
    "LFB_data = pd.read_csv(os.path.join(data_dir, \"lfb_incident.csv\"))\n",
    "\n",
    "# Total memory used\n",
    "print(f'Total Memory Used : {round(LFB_data.memory_usage(deep=True).sum()/(1024*1024), 2)} MB')\n",
    "LFB_data.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd162c7e",
   "metadata": {},
   "source": [
    "# %pip install plotly\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.density_mapbox(LFB_data, lat='Latitude', lon='Longitude', radius=10,\n",
    "                       center = dict(lat = 50, lon= 0.5), zoom=3,\n",
    "                       mapbox_style='stamen-terrain')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63987faa",
   "metadata": {},
   "source": [
    "### Primary Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1224ad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "LFB_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2c41d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LFB_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183fd70a",
   "metadata": {},
   "source": [
    "We can see that there are missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc32933",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d294be60",
   "metadata": {},
   "source": [
    "#### Time Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da44f8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to drop the existing Hour of call and create our own\n",
    "LFB_data.drop('HourOfCall', axis=1, inplace=True)\n",
    "\n",
    "# Create a new column from the DateOfCall column.\n",
    "LFB_data['YearOfCall'], LFB_data['MonthOfCall'] = LFB_data['DateOfCall'].apply(lambda x: x.split(\"-\")[2]),LFB_data['DateOfCall'].apply(lambda x: x.split(\"-\")[1])\n",
    "LFB_data['HourOfCall'] = LFB_data['TimeOfCall'].apply(lambda x: x.split(\":\")[0])\n",
    "\n",
    "# Dropping unnecessary time columns\n",
    "LFB_data.drop(['IncidentNumber','TimeOfCall','DateOfCall','CalYear'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c294a9f",
   "metadata": {},
   "source": [
    "#### Service and group processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cb7f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to turn the incident group into ordinal encoding\n",
    "\n",
    "# We convert the column from object to a category \n",
    "LFB_data['IncidentGroup'] = LFB_data['IncidentGroup'].astype('category')\n",
    "LFB_data['IncidentGroup'] = LFB_data['IncidentGroup'].cat.codes # False alarm = 0, Fire = 1, special service = 2\n",
    "\n",
    "# Drop the columns\n",
    "LFB_data.drop(['StopCodeDescription','SpecialServiceType'], axis=1, inplace = True)\n",
    "LFB_data.sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5058c8",
   "metadata": {},
   "source": [
    "#### Inc Geo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dfe62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inc Geo borough name and code all describe the same borough. \n",
    "\n",
    "# Number of distinct rows for each column\n",
    "print(LFB_data[['IncGeo_BoroughCode', 'IncGeo_BoroughName']].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c9edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can join the two columns into one to avoid to many vatiables after encoding\n",
    "LFB_data['IncGeo_Borough'] = LFB_data['IncGeo_BoroughCode'] + '-' +LFB_data['IncGeo_BoroughName']\n",
    "\n",
    "# Finally we need to drop the two columns\n",
    "LFB_data.drop(['IncGeo_BoroughCode','IncGeo_BoroughName'], axis=1, inplace=True)\n",
    "LFB_data['IncGeo_Borough'].tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea701c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similary for IncGeo code, ward name and ward name new, all describe the same ward\n",
    "\n",
    "# Number of distinct rows for each column\n",
    "print(LFB_data[['IncGeo_WardCode', 'IncGeo_WardName', 'IncGeo_WardNameNew']].nunique())\n",
    "LFB_data['IncGeo_Ward'] = LFB_data['IncGeo_WardCode'] + '-' +LFB_data['IncGeo_WardName']\n",
    "\n",
    "# Drop Unecessary Columns\n",
    "LFB_data.drop(['IncGeo_WardCode','IncGeo_WardName','IncGeo_WardNameNew'], axis=1, inplace=True)\n",
    "LFB_data['IncGeo_Ward'].sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4362ce76",
   "metadata": {},
   "source": [
    "#### Slicing Northing_m and easting_m to 4 last digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d9adc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process Northing and easting data and process it ito categorical data using pandas cut\n",
    "\n",
    "LFB_data['Easting_rounded']= LFB_data['Easting_rounded'].apply(lambda x: int(str(x)[2:]))\n",
    "LFB_data['Easting_rounded']= pd.cut(LFB_data['Easting_rounded'], bins=10, labels=['Easting_rounded_0','Easting_rounded_1','Easting_rounded_2','Easting_rounded_3','Easting_rounded_4','Easting_rounded_5','Easting_rounded_6','Easting_rounded_7','Easting_rounded_8','Easting_rounded_9'])\n",
    "\n",
    "LFB_data['Northing_rounded']= LFB_data['Northing_rounded'].apply(lambda x: int(str(x)[2:]))\n",
    "LFB_data['Northing_rounded']= pd.cut(LFB_data['Northing_rounded'], bins=10, labels=['Northing_rounded_0','Northing_rounded_1','Northing_rounded_2','Northing_rounded_3','Northing_rounded_4','Northing_rounded_5','Northing_rounded_6','Northing_rounded_7','Northing_rounded_8','Northing_rounded_9'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b0cb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "LFB_data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf22330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8442af3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grouping_labels = LFB_data['IncidentGroup']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba39e29",
   "metadata": {},
   "source": [
    "#### Dropping Unecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d635fc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LFB_data.drop(['IncidentGroup','Postcode_full','SecondPumpArriving_AttendanceTime','SecondPumpArriving_DeployedFromStation'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e0f45f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b65e825",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50213c9",
   "metadata": {},
   "source": [
    "### Numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf04243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select numeric columns\n",
    "df_numeric = LFB_data.select_dtypes(include=[np.number])\n",
    "\n",
    "print(\"Numeric data shape : \",df_numeric.shape)\n",
    "df_numeric.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ebbe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric.describe().T.apply(lambda s: s.apply('{0:.2f}'.format))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e37f8a2",
   "metadata": {},
   "source": [
    "We can tell from the data above that there are outliers in the numeric data.\n",
    "For instance, there are values that have a very huge diffrence between the 75th percentile and maximum value\n",
    "\n",
    "#### Numerical data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0087298",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize= (20,50))\n",
    "\n",
    "for i in range(len(df_numeric.columns)):\n",
    "    column = df_numeric.columns[i]\n",
    "    sub = fig.add_subplot(9,3, i+1)\n",
    "    chart = sns.boxplot(data=df_numeric, y=column, x = grouping_labels)\n",
    "    chart.set_title(column + \" by incident group\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89a768b",
   "metadata": {},
   "source": [
    "#### Missing values on Numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4441ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any missing values\n",
    "print(\"Number of cols with Missing Vals: \",df_numeric.isna().any().sum())\n",
    "display(df_numeric.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc3728c",
   "metadata": {},
   "source": [
    "<h6>We need to fix the missing values to cluster around the mean value<br>We will consider randomizing the missing values between 30% and 70%</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4998b4f4",
   "metadata": {},
   "source": [
    "#### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272b3fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i in df_numeric[['UPRN', 'USRN', 'Easting_m', 'Northing_m','Easting_rounded', 'Northing_rounded',\n",
    "#                     'FirstPumpArriving_AttendanceTime','SecondPumpArriving_AttendanceTime',\n",
    "#                     'NumStationsWithPumpsAttending', 'NumPumpsAttending', 'PumpCount',\n",
    "#                     'PumpHoursRoundUp', 'Notional Cost (£)', 'NumCalls']]:\n",
    "for i in df_numeric.columns.values:\n",
    "    df_numeric.fillna(0, inplace=True)\n",
    "    # Set 30 and 70th percentile and round off to 2\n",
    "    rand_30_70 = random.uniform(round(np.percentile(df_numeric[i],30),2), round(np.percentile(df_numeric[i],70),2)) \n",
    "    for j in i:\n",
    "        if j == 0:\n",
    "            df_numeric.replace(to_replace=0, value=rand_30_70, inplace=True)\n",
    "df_numeric.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d17128",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673c13c1",
   "metadata": {},
   "source": [
    "#### Outliers in numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b10104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_numeric.columns:\n",
    "    df_numeric.fillna(df_numeric[i].mode()[0], inplace = True)\n",
    "    highest_val = df_numeric[i].mean() + 3*df_numeric[i].std()\n",
    "    lowest_val = df_numeric[i].mean() - 3*df_numeric[i].std()\n",
    "    print(f\"Range for {i} : \", round(lowest_val,2), \" to \",round(highest_val,2))\n",
    "    \n",
    "#     Trimming the outliers\n",
    "    df_numeric[i]= np.where(df_numeric[i]>highest_val, highest_val,\n",
    "                           np.where(df_numeric[i]<lowest_val, lowest_val,\n",
    "                                   df_numeric[i]))\n",
    "#     (df_numeric[i]>=lowest_val)&(df_numeric[i]<=highest_val)\n",
    "\n",
    "print( \"\\n\",\"*\"*120)\n",
    "df_numeric.describe().T.apply(lambda s: s.apply('{0:.2f}'.format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d20c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c183ec33",
   "metadata": {},
   "source": [
    "### Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b98d2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical = LFB_data.select_dtypes(exclude=[np.number])\n",
    "print(df_categorical.shape)\n",
    "print( \"\\n\",\"-\"*120)\n",
    "df_categorical.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b696c684",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41afb77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show unique values for each categorcal variable\n",
    "\n",
    "df_categorical.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb2107e",
   "metadata": {},
   "source": [
    "### Joining dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55231b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([df_numeric, df_categorical], axis = 1)\n",
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9cd293",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.sample(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ae0588",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b56fe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.get_dummies(final_df)\n",
    "final_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f6a49b",
   "metadata": {},
   "source": [
    "#### Adding the grouping data (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153d9261",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([grouping_labels, final_df], axis=1)\n",
    "final_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514bb3af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bb0c212",
   "metadata": {},
   "source": [
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10d712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)\n",
    "rand_num=random.sample(range(len(final_df)), 10000)\n",
    "rand_num[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae794849",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df = final_df.iloc[rand_num]\n",
    "working_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87528b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9102439",
   "metadata": {},
   "source": [
    "## T-Stochastic Neighbor Embedding (t-SNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e00612",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import scipy\n",
    "import time\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy import stats\n",
    "# A=scipy.spatial.distance.pdist(working_df, metric='euclidean')\n",
    "# kendTSNE=[]\n",
    "\n",
    "start_time = time.time()\n",
    "tsne = TSNE(n_components = 2, verbose=1, learning_rate=200, n_iter=500)\n",
    "tsne_result = tsne.fit_transform(X=working_df)\n",
    "end_time = time.time()\n",
    "print(\"Learning completed in {} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6cd2986",
   "metadata": {},
   "source": [
    "for k in len(working_df):\n",
    "    embed = TSNE(n_jobs=8, perplexity=k,n_components=2)\n",
    "    X_tsne = embed.fit_transform(working_df)\n",
    "    B=scipy.spatial.distance.pdist(X_tsne, metric='euclidean')\n",
    "    kendTSNE.append(scipy.stats.kendalltau(A, B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81418556",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df = pd.DataFrame({\"t-SNE 1\":tsne_result[:,0], \"t-SNE 2\":tsne_result[:,1], \"label\":working_df['grouping_labels']})\n",
    "tsne_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab377ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the tsne data\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(15,10))\n",
    "sns.scatterplot(x = 't-SNE 1', y= 't-SNE 2', data = tsne_df, hue='label', ax = ax, s=20, palette = 'dark')\n",
    "sns.color_palette(\"hls\", 10)\n",
    "lim = (tsne_df.min()-5, tsne_df.max()+5)\n",
    "ax.set_title('t-SNE Visualization of Incident Group', fontsize = 16, weight = 'bold')\n",
    "ax.legend(bbox_to_anchor = (1,1), loc =2, borderaxespad = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fff91a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36e40502",
   "metadata": {},
   "source": [
    "## Same Degree Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94520a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.sparse import csr_matrix, issparse\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils._openmp_helpers import _openmp_effective_n_threads\n",
    "from sklearn.utils.validation import check_non_negative\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "MACHINE_EPSILON = np.finfo(np.double).eps"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d8c78cf",
   "metadata": {},
   "source": [
    "def _joint_probabilities(X, degrees_of_freedom):\n",
    "    dist = pdist(X, \"sqeuclidean\")\n",
    "    dist += 1.\n",
    "    dist **= (-degrees_of_freedom)\n",
    "    P= np.maximum(dist / ( np.sum(dist)), MACHINE_EPSILON)\n",
    "    return P\n",
    "\n",
    "\n",
    "def _kl_divergence(params, P, degrees_of_freedom, n_samples, n_components,\n",
    "                  skip_num_points=0, compute_error=True):\n",
    "\n",
    "    X_embedded = params.reshape(n_samples, n_components)\n",
    "    dist = pdist(X_embedded, \"sqeuclidean\")\n",
    "    Dist=dist+ 1.\n",
    "    dist= Dist**(-degrees_of_freedom)\n",
    "    Q = np.maximum(dist / (np.sum(dist)), MACHINE_EPSILON)\n",
    "    dist1=Dist**(-1)\n",
    "    kl_divergence =np.dot(P, np.log(np.maximum(P, MACHINE_EPSILON) / Q))\n",
    "    grad = np.ndarray((n_samples, n_components), dtype=params.dtype)\n",
    "    PQd = squareform((P - Q) * dist1)\n",
    "    for i in range(skip_num_points, n_samples):\n",
    "        grad[i] = np.dot(np.ravel(PQd[i]),\n",
    "                        X_embedded[i] - X_embedded)\n",
    "    grad = grad.ravel()\n",
    "    c = 2*(degrees_of_freedom + 1.0) \n",
    "    grad *= c\n",
    "\n",
    "    return kl_divergence, grad\n",
    "\n",
    "def _gradient_descent(objective, p0, it, n_iter,\n",
    "                     n_iter_check=1, n_iter_without_progress=300,\n",
    "                     momentum=0.5, learning_rate=7, min_gain=0.01,\n",
    "                     min_grad_norm=1e-7, verbose=0, args=None, kwargs=None):\n",
    "    if args is None:\n",
    "        args = []\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "\n",
    "    p = p0.copy().ravel()\n",
    "    update = np.zeros_like(p)\n",
    "    gains = np.ones_like(p)\n",
    "    error = np.finfo(np.float).max\n",
    "    best_error = np.finfo(np.float).max\n",
    "    best_iter = i = it\n",
    "\n",
    "    tic = time()\n",
    "    for i in range(it, n_iter):\n",
    "        check_convergence = (i + 1) % n_iter_check == 0\n",
    "       \n",
    "        kwargs['compute_error'] = check_convergence or i == n_iter - 1\n",
    "\n",
    "        error, grad = objective(p, *args, **kwargs)\n",
    "        grad_norm = linalg.norm(grad)\n",
    "\n",
    "        inc = update * grad < 0.0\n",
    "        dec = np.invert(inc)\n",
    "        gains[inc] += 0.2\n",
    "        gains[dec] *= 0.8\n",
    "        np.clip(gains, min_gain, np.inf, out=gains)\n",
    "        grad *= gains\n",
    "        update = momentum * update - learning_rate * grad\n",
    "        p += update\n",
    "\n",
    "        if check_convergence:\n",
    "            toc = time()\n",
    "            duration = toc - tic\n",
    "            tic = toc\n",
    "\n",
    "            if verbose >= 2:\n",
    "                print(\"[t-SNE] Iteration %d: error = %.7f,\"\n",
    "                     \" gradient norm = %.7f\"\n",
    "                     \" (%s iterations in %0.3fs)\"\n",
    "                     % (i + 1, error, grad_norm, n_iter_check, duration))\n",
    "\n",
    "            if error < best_error:\n",
    "                best_error = error\n",
    "                best_iter = i\n",
    "            elif i - best_iter > n_iter_without_progress:\n",
    "                if verbose >= 2:\n",
    "                    print(\"[t-SNE] Iteration %d: did not make any progress \"\n",
    "                         \"during the last %d episodes. Finished.\"\n",
    "                         % (i + 1, n_iter_without_progress))\n",
    "                break\n",
    "            if grad_norm <= min_grad_norm:\n",
    "                if verbose >= 2:\n",
    "                    print(\"[t-SNE] Iteration %d: gradient norm %f. Finished.\"\n",
    "                         % (i + 1, grad_norm))\n",
    "                break\n",
    "\n",
    "    return p, error, i\n",
    "\n",
    "\n",
    "class SDD(BaseEstimator):\n",
    "   \n",
    "   \n",
    "    _EXPLORATION_N_ITER = 300\n",
    "    _N_ITER_CHECK =50\n",
    "    def _fit(self, X, degrees_of_freedom, skip_num_points=0):\n",
    "        random_state=None\n",
    "        n_samples=X.shape[0]\n",
    "        P = _joint_probabilities(X,degrees_of_freedom)\n",
    "\n",
    "        random_state = check_random_state(random_state)\n",
    "\n",
    "        X_embedded = 1e-4 * random_state.randn(\n",
    "               n_samples, 2).astype(np.float32)\n",
    "\n",
    "        return self._tsne(P, degrees_of_freedom, n_samples,X_embedded=X_embedded,skip_num_points=skip_num_points)\n",
    "    def _tsne(self, P, degrees_of_freedom, n_samples, X_embedded, skip_num_points=0):\n",
    "        \"\"\"Runs t-SNE.\"\"\"\n",
    "        params = X_embedded.ravel()\n",
    "\n",
    "        opt_args = {\n",
    "           \"it\": 0,\n",
    "           \"n_iter_check\": 50,\n",
    "           \"min_grad_norm\": 1e-7,\n",
    "           \"learning_rate\": 7,\n",
    "           \"verbose\": 0,\n",
    "           \"kwargs\": dict(skip_num_points=skip_num_points),\n",
    "           \"args\": [P, degrees_of_freedom, n_samples, 2],\n",
    "           \"n_iter_without_progress\": 300,\n",
    "           \"n_iter\": 300,\n",
    "           \"momentum\": 0.8,\n",
    "       }\n",
    "        obj_func = _kl_divergence\n",
    "\n",
    "        params, kl_divergence, it = _gradient_descent(obj_func, params,\n",
    "                                                     **opt_args)\n",
    "        P /= 1\n",
    "        remaining =2000 - 300\n",
    "        if it < 300 or remaining > 0:\n",
    "            opt_args['n_iter'] = 2000\n",
    "            opt_args['it'] = it + 1\n",
    "            opt_args['momentum'] = 0.8\n",
    "            opt_args['n_iter_without_progress'] = 300\n",
    "            params, kl_divergence, it = _gradient_descent(obj_func, params,\n",
    "                                                         **opt_args)\n",
    "\n",
    "        self.n_iter_ = it\n",
    "        X_embedded = params.reshape(n_samples, 2)\n",
    "        self.kl_divergence_ = kl_divergence\n",
    "\n",
    "        return X_embedded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64f6a55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "04b2e48da8715d0e03ed003238e610aa18fc3abec3b418b1e66df75f2c0e936e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
